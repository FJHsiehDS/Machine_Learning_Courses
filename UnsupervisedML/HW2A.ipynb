{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## PROBLEM 1: KMeans Theory"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84a251a507820f66"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Kmeans Objective: min(sum(sum(pi_ik * ||X_i - M_k||^2))), where pi_ik = 1 if X_i is assigned to cluster k, 0 otherwise.\n",
    "(A) Proof E step(updating membership minimize the objective)\n",
    "        Given the current centroids, we can find the closest M_k to X_i and assign the X_i to the cluster making the objective minimum.\n",
    "(B) Proof M step(updating centroids minimize the objective)\n",
    "       Given the current membership, we can calculate the derivative of the objective function with respect to the centroids and set it to zero to find the optimal centroids. \n",
    "       After differentiation, the objective function can be written as sum(pi_ik * (2M_k - 2X_i)) = 0, \n",
    "                                                                                                        M_k * sum(pi_ik) = sum(pi_ik*X_i)\n",
    "                                                                                                        M_k = sum(pi_ik*X_i)/sum(pi_ik).\n",
    "       Therefore, the solution of M_k to minimize the objective function is the average of the data points in the cluster k.   \n",
    "(C) By the proof of (A) and (B), the iteration of the KMeans algorithm make the objective function decrease or unchanged monotonically. And it will converge to a point when no further decrease is possible. The E and M steps might converge at a local minimum, which also fulfills the goal of no further decrease."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "324cd4e1d986957e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PROBLEM 2 : KMeans on data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8d857e3c6d7290d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T04:28:14.964568400Z",
     "start_time": "2025-02-03T04:28:14.854614500Z"
    }
   },
   "id": "467c7fb89a583e50",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def initialize_centroids(X, K):\n",
    "    '''\n",
    "    randomly select k data points as the initial centroids\n",
    "    :param X: data\n",
    "    :param K: number of clusters\n",
    "    :return: list of K centroids\n",
    "    '''\n",
    "    np.random.seed(17)\n",
    "    # randomly select K indices, use X[indices] as the initial centroids\n",
    "    indices = np.random.choice(X.shape[0], K, replace=False) \n",
    "    return X[indices]\n",
    "\n",
    "def hard_kmeans(X, K, max_iter = 100, tol = 1e-4):\n",
    "    '''\n",
    "    :param X: data\n",
    "    :param K: number of clusters\n",
    "    :param max_iter: maximum number of iterations\n",
    "    :param tol: tolerance for stopping criterion\n",
    "    :return:  assignments, centroids, objective function value\n",
    "    '''\n",
    "    centroids = initialize_centroids(X, K)\n",
    "    for i in range(max_iter):\n",
    "        # E step\n",
    "        dist = pairwise_distances(X, centroids)\n",
    "        assignments = np.argmin(dist, axis=1)\n",
    "        # M step\n",
    "        new_centroids = np.array([X[assignments == k].mean(axis=0) for k in range(K)])\n",
    "        if np.linalg.norm(new_centroids - centroids) < tol:\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "\n",
    "    obj = np.sum((X - centroids[assignments])**2)\n",
    "    return assignments, centroids, obj\n",
    "\n",
    "\n",
    "def normalize_images(X):\n",
    "    normalized_images = []\n",
    "    X = X.reshape(X.shape[0], X.shape[1]*X.shape[2])\n",
    "    for image in X:\n",
    "        normalized_images.append((image - np.min(image))/np.max(image))\n",
    "    return np.array(normalized_images)\n",
    "\n",
    "def soft_kmeans(X, K, beta, max_iter = 100, tol = 1e-4):\n",
    "    '''\n",
    "    :param X: data\n",
    "    :param K: number of clusters\n",
    "    :param beta: parameter for soft kmeans\n",
    "    :param max_iter: maximum number of iterations\n",
    "    :param tol: tolerance for stopping criterion\n",
    "    :return:  assignments, centroids,objective function value\n",
    "    '''\n",
    "    if np.isnan(X).any():\n",
    "        means = np.nanmean(X, axis=0)\n",
    "        X = np.where(np.isnan(X), means, X) # replace NaN with the mean of the column\n",
    "    centroids = initialize_centroids(X, K)\n",
    "    for i in range(max_iter):\n",
    "        # E step\n",
    "        dist = pairwise_distances(X, centroids)\n",
    "        assignments = np.exp(-beta * dist)\n",
    "        assignments /= (assignments.sum(axis=1)[:, np.newaxis]+ 1e-10)\n",
    "        # M step\n",
    "        new_centroids = np.dot(assignments.T, X) / (assignments.sum(axis=0)[:, np.newaxis]  + 1e-10)\n",
    "        if np.linalg.norm(new_centroids - centroids) < tol:\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "    obj = np.sum(np.sum(assignments * dist))\n",
    "    return assignments, centroids, obj\n",
    "\n",
    "def purity(assignments, true_labels, K):\n",
    "    '''\n",
    "    external measure for clustering by purity:\n",
    "    formula: sum()/n\n",
    "    :param assignments: cluster assignments\n",
    "    :param true_labels: true labels\n",
    "    :return: purity\n",
    "    '''\n",
    "    n = len(assignments)\n",
    "    purity = 0\n",
    "    for k in range(K):\n",
    "        indices = np.where(assignments == k)[0]\n",
    "        if len(indices) == 0:\n",
    "            continue\n",
    "        counts = np.bincount(true_labels[indices]) # count the occurrence of each label\n",
    "        purity += np.max(counts) # count of the most frequent label\n",
    "    return purity / n\n",
    "\n",
    "def gini_index(assignments, true_labels, K):\n",
    "    '''\n",
    "    external measure for clustering by gini index:\n",
    "    formula: 1 - sum(p_k * (1 - sum(p_i^2))), where is probability distribution of the true labels within a given cluster k\n",
    "    :param assignments: cluster assignments\n",
    "    :param true_labels: true labels\n",
    "    :return: gini index\n",
    "    '''\n",
    "    n = len(assignments)\n",
    "    gini = 0\n",
    "    for k in range(K):\n",
    "        indices = np.where(assignments == k)[0]\n",
    "        if len(indices) == 0:\n",
    "            continue\n",
    "        counts = np.bincount(true_labels[indices]) # count the occurrence of each label\n",
    "        p_k = counts / counts.sum() # proportion of k-th class\n",
    "        gini_k = 1 - np.sum(p_k ** 2)\n",
    "        gini += (len(indices) / n) * gini_k # weight by cluster size\n",
    "    return gini"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T04:12:17.544693300Z",
     "start_time": "2025-02-03T04:12:17.535722600Z"
    }
   },
   "id": "78e7fc483038ffd6",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#A) MNIST Dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_test_norm = normalize_images(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-02T18:54:26.965441600Z",
     "start_time": "2025-02-02T18:54:26.354351700Z"
    }
   },
   "id": "20d5fd54819b35b9",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST Dataset internal measures\n",
      "K=5, Objective:  432282.69567812496\n",
      "K=10, Objective:  392102.3941812986\n",
      "K=20, Objective:  349432.15135823673\n"
     ]
    }
   ],
   "source": [
    "#evaluate the KMeans objective for a higher K (for example double) or smaller K(for example half).\n",
    "print(\"MNIST Dataset internal measures\")\n",
    "assignments_5, _, obj_5 = hard_kmeans(X_test_norm, 5)\n",
    "print(\"K=5, Objective: \", obj_5)\n",
    "assignments_10, _, obj_10 = hard_kmeans(X_test_norm, 10)\n",
    "print(\"K=10, Objective: \", obj_10)\n",
    "assignments_20, _, obj_20 = hard_kmeans(X_test_norm, 20)\n",
    "print(\"K=20, Objective: \", obj_20)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T04:11:36.131178300Z",
     "start_time": "2025-02-03T04:10:48.091963Z"
    }
   },
   "id": "7355dbdc889022b6",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST Dataset external measures\n",
      " K=5, Purity:  0.4525    Gini Index:  0.6585887450133034\n",
      " K=10, Purity:  0.5272    Gini Index:  0.5936870630149979\n",
      " K=20, Purity:  0.7204    Gini Index:  0.38698754685545056\n"
     ]
    }
   ],
   "source": [
    "print(\"MNIST Dataset external measures\")\n",
    "print(\" K=5, Purity: \", purity(assignments_5, y_test, 5),\"  \", \"Gini Index: \", gini_index(assignments_5, y_test, 5))\n",
    "print(\" K=10, Purity: \", purity(assignments_10, y_test, 10),\"  \", \"Gini Index: \", gini_index(assignments_10, y_test, 10))\n",
    "print(\" K=20, Purity: \", purity(assignments_20, y_test, 20),\"  \", \"Gini Index: \", gini_index(assignments_20, y_test, 20))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T04:13:26.551987100Z",
     "start_time": "2025-02-03T04:13:26.422603200Z"
    }
   },
   "id": "a0b2483c27445b8c",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST Dataset soft KMeans\n",
      "Beta=0.1, Objective:  72088.81146261489    Purity:  0.2733    Gini Index:  0.8298867835172213\n",
      "Beta=1, Objective:  72088.81145907445    Purity:  0.2105    Gini Index:  0.869986882809169\n",
      "Beta=10, Objective:  61581.90211387066    Purity:  0.5306    Gini Index:  0.5894889401892156\n"
     ]
    }
   ],
   "source": [
    "print(\"MNIST Dataset soft KMeans\")\n",
    "soft_assignments1, _, obj1 = soft_kmeans(X_test_norm, 10, 0.1)\n",
    "print(\"Beta=0.1, Objective: \", obj1, \"  \", \"Purity: \", purity(np.argmax(soft_assignments1, axis=1), y_test, 10), \"  \", \"Gini Index: \", gini_index(np.argmax(soft_assignments1, axis=1), y_test, 10))\n",
    "soft_assignments2, _, obj2 = soft_kmeans(X_test_norm, 10, 1)\n",
    "print(\"Beta=1, Objective: \", obj2, \"  \", \"Purity: \", purity(np.argmax(soft_assignments2, axis=1), y_test, 10), \"  \", \"Gini Index: \", gini_index(np.argmax(soft_assignments2, axis=1), y_test, 10))\n",
    "soft_assignments3, _, obj3 = soft_kmeans(X_test_norm, 10, 10)\n",
    "print(\"Beta=10, Objective: \", obj3, \"  \", \"Purity: \", purity(np.argmax(soft_assignments3, axis=1), y_test, 10), \"  \", \"Gini Index: \", gini_index(np.argmax(soft_assignments3, axis=1), y_test, 10))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T04:22:52.587988400Z",
     "start_time": "2025-02-03T04:22:45.141103600Z"
    }
   },
   "id": "4649934977649bf0",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "\u001B[1m29515/29515\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "\u001B[1m26421880/26421880\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "\u001B[1m5148/5148\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "\u001B[1m4422102/4422102\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "#B) FASHION Dataset\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_test_norm = normalize_images(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T04:29:00.153334Z",
     "start_time": "2025-02-03T04:28:53.592317700Z"
    }
   },
   "id": "e8c0f70b94be3b59",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000],\n      dtype=int64)"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T05:05:28.956754100Z",
     "start_time": "2025-02-03T05:05:28.898993Z"
    }
   },
   "id": "475b5865ee0d6e65",
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fashion_mnist Dataset measures\n",
      " K=5, Purity:  0.1082    Gini Index:  0.8997557926683448    Objective:  432282.69567812496\n",
      " K=10, Purity:  0.1134    Gini Index:  0.8990142616258867    Objective:  392102.3941812986\n",
      " K=20, Purity:  0.1199    Gini Index:  0.89831345889829    Objective:  349479.4016943277\n"
     ]
    }
   ],
   "source": [
    "print(\"fashion_mnist Dataset measures\")\n",
    "assignments_5, _, obj_5 = hard_kmeans(X_test_norm, 5)\n",
    "print(\" K=5, Purity: \", purity(assignments_5, y_test, 5),\"  \", \"Gini Index: \", gini_index(assignments_5, y_test, 5), \"  \", \"Objective: \", obj_5)\n",
    "assignments_10, _, obj_10 = hard_kmeans(X_test_norm, 10)\n",
    "print(\" K=10, Purity: \", purity(assignments_10, y_test, 10),\"  \", \"Gini Index: \", gini_index(assignments_10, y_test, 10), \"  \", \"Objective: \", obj_10)\n",
    "assignments_20, _, obj_20 = hard_kmeans(X_test_norm, 20)\n",
    "print(\" K=20, Purity: \", purity(assignments_20, y_test, 20),\"  \", \"Gini Index: \", gini_index(assignments_20, y_test, 20), \"  \", \"Objective: \", obj_20)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T04:33:17.505910Z",
     "start_time": "2025-02-03T04:32:40.400418900Z"
    }
   },
   "id": "c689a5c9722816f7",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#C) 20NG Dataset\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "test_data = fetch_20newsgroups(subset='test', shuffle=True, random_state=17)\n",
    "\n",
    "# Apply TF normalization\n",
    "vectorizer = TfidfVectorizer()\n",
    "test_documents_tfidf = vectorizer.fit_transform(test_data.data).toarray()\n",
    "y_test = test_data.target"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T06:06:47.797541300Z",
     "start_time": "2025-02-03T06:06:24.419413100Z"
    }
   },
   "id": "7cd4fc4b15f9ad5f",
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20NG Dataset measures\n",
      " K=10, Purity:  0.21216144450345195    Gini Index:  0.8713301810616431    Objective:  7005.268149600954\n",
      " K=20, Purity:  0.20379713223579396    Gini Index:  0.8611020325434653    Objective:  6961.470507827989\n",
      " K=30, Purity:  0.22039298990971853    Gini Index:  0.8398194867573759    Objective:  6895.8515665928335\n"
     ]
    }
   ],
   "source": [
    "print(\"20NG Dataset measures\")\n",
    "assignments_10, _, obj_10 = hard_kmeans(test_documents_tfidf, 10)\n",
    "print(\" K=10, Purity: \", purity(assignments_10, y_test, 10),\"  \", \"Gini Index: \", gini_index(assignments_10, y_test, 10), \"  \", \"Objective: \", obj_10)\n",
    "assignments_20, _, obj_20 = hard_kmeans(test_documents_tfidf, 20)\n",
    "print(\" K=20, Purity: \", purity(assignments_20, y_test, 20),\"  \", \"Gini Index: \", gini_index(assignments_20, y_test, 20), \"  \", \"Objective: \", obj_20)\n",
    "assignments_30, _, obj_30 = hard_kmeans(test_documents_tfidf, 30)\n",
    "print(\" K=30, Purity: \", purity(assignments_30, y_test, 30),\"  \", \"Gini Index: \", gini_index(assignments_30, y_test, 30), \"  \", \"Objective: \", obj_30)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T06:40:17.747244200Z",
     "start_time": "2025-02-03T06:06:47.887956600Z"
    }
   },
   "id": "6192cf2196e44cd1",
   "execution_count": 67
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PROBLEM 3 : Gaussian Mixture on toy data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5988f4c3dc606818"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "import numpy.linalg as LA"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T15:55:59.978860Z",
     "start_time": "2025-02-03T15:55:59.894771Z"
    }
   },
   "id": "ec4fb73f7f25c3d",
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def initialize_parameters(X, K):\n",
    "    '''\n",
    "    randomly select k data points as the initial centroids\n",
    "    :param X: data\n",
    "    :param K: number of clusters\n",
    "    '''\n",
    "    np.random.seed(17)\n",
    "    N, d = X.shape\n",
    "    weights = np.ones(K) / K\n",
    "    \n",
    "    # choose K random data points as the initial means\n",
    "    indices = np.random.choice(N, K, replace=False)\n",
    "    means = X[indices]\n",
    "    \n",
    "    # initialize covariances.\n",
    "    data_cov = np.cov(X, rowvar=False)\n",
    "    covariances = np.array([data_cov.copy() for _ in range(K)]) # 1 covariance matrix per cluster\n",
    "    return weights, means, covariances\n",
    "\n",
    "def logsumexp(a, axis, keepdims=True):\n",
    "    '''\n",
    "    computing the log-sum-exp of the array 'a' in a numerically stable way\n",
    "    '''\n",
    "    a_max = np.max(a,axis = axis, keepdims=True)\n",
    "    s = np.log(np.sum(np.exp(a - a_max), axis=axis, keepdims=True))\n",
    "    if not keepdims:\n",
    "        s = np.squeeze(s, axis=axis)\n",
    "        a_max = np.squeeze(a_max, axis=axis)\n",
    "    return a_max + s\n",
    "\n",
    "def gmm(X, K, max_iter=100, tol = 1e-6, eps=1e-6):\n",
    "    N,d = X.shape\n",
    "    weights, means, covariances = initialize_parameters(X, K)\n",
    "    log_likelihoods = []\n",
    "    for i in range(max_iter):\n",
    "        #E-step\n",
    "        log_resp = np.zeros((N,K))\n",
    "        for k in range(K):\n",
    "            #add small constant to the diagonal\n",
    "            cov_k = covariances[k] + np.eye(d) * eps\n",
    "            #compute log probability for each point under the k-th Gaussian\n",
    "            log_pdf = multivariate_normal.logpdf(X, mean=means[k], cov=cov_k)\n",
    "            log_resp[:,k] = np.log(weights[k]) + log_pdf\n",
    "            \n",
    "        #use log-sum-exp to get the log likelihood and normalize resp\n",
    "        log_sum = logsumexp(log_resp,axis=1,keepdims=True)\n",
    "        log_likelihood = np.sum(log_sum)\n",
    "        log_likelihoods.append(log_likelihood)\n",
    "        \n",
    "        #convert log resp to actual resp\n",
    "        resp = np.exp(log_resp - log_sum) #shape (N,K)\n",
    "        \n",
    "        #M-step\n",
    "        Nk = resp.sum(axis=0) # effective number of points per component, shape (K,)\n",
    "        weights = Nk/N \n",
    "        new_means = np.zeros((K, d))\n",
    "        new_covariances = np.zeros((K, d, d))\n",
    "        for k in range(K):\n",
    "            new_means[k] = (resp[:,k][:,np.newaxis] * X).sum(axis=0) / Nk[k]\n",
    "            diff = X - new_means[k]\n",
    "            new_covariances[k] = (resp[:, k][:, np.newaxis] * diff).T.dot(diff) / Nk[k]\n",
    "            new_covariances[k] += np.eye(d) * eps\n",
    "        means, covariances = new_means, new_covariances\n",
    "        \n",
    "        #convergence check\n",
    "        if i > 0 and np.abs(log_likelihood - log_likelihoods[-2]) < tol:\n",
    "            print(f\"Converged after {i} iterations\")\n",
    "            break\n",
    "        \n",
    "    return weights, means, covariances, log_likelihoods"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T20:12:07.083014100Z",
     "start_time": "2025-02-03T20:12:07.038921900Z"
    }
   },
   "id": "53dc306c9d1e94e7",
   "execution_count": 94
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 26 iterations\n",
      "Weights:  [0.66520194 0.33479806]\n",
      "Means:  [[7.0131553  3.98313827]\n",
      " [2.99414548 3.05209487]]\n",
      "Covariances:  [[[0.97474789 0.49746329]\n",
      "  [0.49746329 1.00113766]]\n",
      "\n",
      " [[1.01025806 0.02718868]\n",
      "  [0.02718868 2.93781334]]]\n"
     ]
    }
   ],
   "source": [
    "g2 = np.loadtxt(\"dataset/2gaussian.txt\")\n",
    "weights, means, covariances, log_likelihoods = gmm(g2, 2)\n",
    "print(\"Weights: \", weights)\n",
    "print(\"Means: \", means)\n",
    "print(\"Covariances: \", covariances)\n",
    "#print(\"Log likelihoods: \", log_likelihoods)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T20:13:57.811518100Z",
     "start_time": "2025-02-03T20:13:57.735214Z"
    }
   },
   "id": "10baf2219fdf8e13",
   "execution_count": 97
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 73 iterations\n",
      "Weights:  [0.29843544 0.20560498 0.49595958]\n",
      "Means:  [[7.0215704  4.01546579]\n",
      " [3.03973175 3.04859572]\n",
      " [5.01174254 7.0014849 ]]\n",
      "Covariances:  [[[0.99039914 0.50095338]\n",
      "  [0.50095338 0.99564928]]\n",
      "\n",
      " [[1.02853436 0.02689709]\n",
      "  [0.02689709 3.38491062]]\n",
      "\n",
      " [[0.97969752 0.18514711]\n",
      "  [0.18514711 0.97452846]]]\n"
     ]
    }
   ],
   "source": [
    "g3 = np.loadtxt(\"dataset/3gaussian.txt\")\n",
    "weights, means, covariances, log_likelihoods = gmm(g3, 3)\n",
    "print(\"Weights: \", weights)\n",
    "print(\"Means: \", means)\n",
    "print(\"Covariances: \", covariances)\n",
    "#print(\"Log likelihoods: \", log_likelihoods)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T20:14:03.717812600Z",
     "start_time": "2025-02-03T20:13:58.235600Z"
    }
   },
   "id": "c65357127ef8ac1",
   "execution_count": 98
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PROBLEM 4 EM for coin flips"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52edd045d1594de4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from scipy.special import comb, logsumexp\n",
    "\n",
    "def binomial_mixture(heads, D, K, max_iter=100, tol=1e-6):\n",
    "    N = len(heads)\n",
    "    np.random.seed(17)\n",
    "    #ensures sum(pi) = 1\n",
    "    pi = np.random.dirichlet(alpha=np.ones(K)) \n",
    "    #randomly initialize p from 0.2 to 0.8 to avoid extreme values\n",
    "    p = np.random.uniform(0.2,0.8,size=K)\n",
    "    log_likelihoods = []\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        #E-step\n",
    "        logC = np.log(comb(D, heads))\n",
    "        log_prob = np.zeros((N,K))\n",
    "        for k in range(K):\n",
    "            log_prob[:, k] = logC + heads * np.log(p[k]) + (D-heads) * np.log(1-p[k])\n",
    "        log_prob += np.log(pi)\n",
    "        \n",
    "        log_sum = logsumexp(log_prob,axis=1)\n",
    "        log_likelihood = np.sum(log_sum)\n",
    "        log_likelihoods.append(log_likelihood)\n",
    "        \n",
    "        #compute reponsibilities: r_ik = exp(log_prob - log_sum_i)\n",
    "        resp = np.exp(log_prob - log_sum[:,None])\n",
    "        \n",
    "        #M-step\n",
    "        #effective number of sessions assigned to each coin, shape (K,)\n",
    "        Nk = resp.sum(axis=0)\n",
    "        pi = Nk/N\n",
    "        \n",
    "        #p_k = (weighted sum of heads) / (D * effective sessions)\n",
    "        for k in range(K):\n",
    "            p[k] = np.sum(resp[:,k] * heads) / (D * Nk[k])\n",
    "        \n",
    "        #convergence check\n",
    "        if i > 0 and np.abs(log_likelihoods[-1] - log_likelihoods[-2]) < tol:\n",
    "            print(f\"Converged after {i} iterations\")\n",
    "            break\n",
    "            \n",
    "    return pi, p, log_likelihoods\n",
    "        \n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T23:21:56.865710500Z",
     "start_time": "2025-02-03T23:21:56.650675600Z"
    }
   },
   "id": "b7c0e60fec3af58f",
   "execution_count": 120
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 22 iterations\n",
      "Estimated coin selection probabilities (pi): [0.30679528 0.17855886 0.51464586]\n",
      "Estimated coin head biases (p): [0.23690868 0.93172817 0.61002961]\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(\"dataset/coin_flips_outcome.txt\",dtype=int)\n",
    "heads = np.sum(data,axis=1)\n",
    "D = data.shape[1]\n",
    "pi, p, log_likelihoods = binomial_mixture(heads, D, 3)\n",
    "print(\"Estimated coin selection probabilities (pi):\", pi)\n",
    "print(\"Estimated coin head biases (p):\", p)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-03T23:22:05.530735500Z",
     "start_time": "2025-02-03T23:22:05.418809300Z"
    }
   },
   "id": "5cd73eb654a9b72d",
   "execution_count": 121
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1.22777875e+00, -4.73712915e-01],\n       [-1.00000000e+00,  1.22464680e-16],\n       [ 1.90711910e+00,  7.91259922e-02],\n       ...,\n       [ 6.21378005e-01,  7.83510928e-01],\n       [ 5.34885651e-02,  9.98568462e-01],\n       [ 2.85145667e-01, -1.99273396e-01]])"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "x, y = make_moons(n_samples=1000, random_state=17)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-15T04:01:32.423134200Z",
     "start_time": "2025-02-15T04:01:22.588512600Z"
    }
   },
   "id": "2fc0a75db80ffecd",
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
